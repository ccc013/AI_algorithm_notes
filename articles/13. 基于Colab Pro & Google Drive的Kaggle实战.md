# 13. 基于Colab Pro & Google Drive的Kaggle实战

原文：https://hippocampus-garden.com/kaggle_colab/

原文标题：How to Kaggle with Colab Pro & Google Drive

译文作者：kbsc13

联系方式：

Github：https://github.com/ccc013/AI_algorithm_notes

知乎专栏：[机器学习与计算机视觉](https://www.zhihu.com/column/c_1060581544644718592)，[AI 论文笔记](https://www.zhihu.com/column/c_1364201355796656128)

微信公众号：**AI 算法笔记**

![](https://img-blog.csdnimg.cn/20210620155918137.jpeg#pic_center)



## 前言

Colab Pro(目前仅在美国、加拿大、日本、巴西、德国、法国、印度、英国和泰国可用)提供了随时可用和加速但是维护起来既昂贵又繁琐的云计算资源。和其免费版不同，Colab Pro 允许用户使用 TPUs 和高端的 GPUs，比如 V100 和 P100 等等，可以访问高内存的实例，并且保持 notebooks 运行的时间最长可以达到 24 小时，费用是每个月 10 美元。

Colab Pro可以满足 Kaggle 竞赛中大部分比赛的资源要求。但是这里存在一个问题，每个会话只能保持 24 小时。每次都需要准备数据集，根据准备的方式，这需要一些时间。在下面的表格中，从初始化加载和磁盘读写的时间来对比 5 种准备 Kaggle 数据集的方法：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/colab_pro_google_drive_1.png)

不幸的是，根据上述表格并没有看到两个方式都很快速的方法。考虑到我们更想要的是在数据集上通过多次迭代的训练模型，快速的磁盘读写的速度是更加重要。在目前的状况下，我会选择第三个方法：首先通过 Kaggle API 下载数据集，并以 zip 压缩包形式保存在 Google 硬盘上，当开始会话的时候，解压缩并存放到实例的磁盘上。这个操作过程将在下一节中一步步的解释。

## Kaggle on Colab Pro

### 下载数据集到 Google 硬盘上

首先，需要通过 Kaggle API 下载数据集，并以 zip 压缩包形式保存在 Google 硬盘上，具体步骤如下所示：

1. 登录到 `https://www.kaggle.com/<YourKaggleID>/account`，然后下载 `kaggle.json`

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/colab_pro_google_drive_2.png)

2. 在Google 硬盘上创建一个名字为 `kaggle` 的文件夹，然后上传 `kaggle.json` 
3. 开始一个 Colab 的会话
4. 通过点击右上角的图标来挂载 Google 硬盘，如下图所示

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/colab_pro_google_drive_3.png)

5. 从 Google 硬盘里复制 `kaggle.json` 到当前会话，并修改文件的权限，命令如下所示：

```
! mkdir -p ~/.kaggle
! cp ./drive/MyDrive/kaggle/kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
```

6. （可选）升级 Kaggle API。这个包是预安装在 Colab 的实例上的，不过在 2021 年 5 月份，Kaggle notebooks 的版本是更新了，两个版本是有些不一样的。

```
! pip install -U kaggle
```

7. 通过 Kaggle API 下载数据集到 Google 硬盘上，这可能会花点时间完成并且在 Google 硬盘界面上显示出来也需要几分钟。

```
! mkdir -p ./drive/MyDrive/kaggle/<CompetitionID>
! kaggle competitions download -c <CompetitionID> -p ./drive/MyDrive/kaggle/<CompetitionID>
```

你也可以升级你的 Google 硬盘计划来获取更多的存储空间。

### 解压缩文件到实例上

解压缩文件到当前会话中，命令如下所示，这一步也需要花费一些时间：

```
! mkdir -p <CompetitionID>
! unzip -q ./drive/MyDrive/kaggle/<CompetitionID>.zip -d <CompetitionID>
# You can specify the portion of dataset for saving time and disk space
! unzip -q ./drive/MyDrive/kaggle/<CompetitionID>.zip train/* -d <CompetitionID>
```

这就可以开始训练模型了。完成训练后，可以将权重文件导出到Kaggle数据集，并通过Kaggle API提交预测，对于完整的教程，可以参考 https://github.com/Kaggle/kaggle-api。



### 速度比较

从Google 硬盘压缩需要很长的时间，这种方法是真的比直接通过 Kaggle API 或者 gsutil 下载要更快吗？为了回答这个问题，我准备了房价预测竞赛（https://www.kaggle.com/c/house-prices-advanced-regression-techniques/）的数据集，大约是 935KB，并测试这 3 种方法的解压缩时间，结果如下所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/colab_pro_google_drive_4.png)

上述结果可能会受到实例所在区域而有些影响，但是大部分情况下，从 Google 硬盘上解压缩还是最快的方法。



### 注意磁盘大小

Colab Pro 目前提供一个 150GB 大小的磁盘，所以压缩文件不能超过 75GB。



## 是否可以挂载外部存储器呢？

### 挂载谷歌云存储桶(Mounting Google Cloud Storage Buckets)

Colab 可以挂载 Google 云存储磁盘，并在不用下载的情况下访问 Kaggle 的数据集。其操作有几个步骤，首先，通过下述代码授权你的账户：

```python
from google.colab import auth
auth.authenticate_user()
```

接着，安装 `gcsfuse` ：

```
! echo "deb http://packages.cloud.google.com/apt gcsfuse-bionic main" > /etc/apt/sources.list.d/gcsfuse.list
! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
! apt update
! apt install gcsfuse
```

然后，打开一个你喜欢的竞赛项目的 Kaggle 的 notebook，然后运行下面的代码获取 GCS （Google Cloud Storage）的路径：

```python
from kaggle_datasets import KaggleDatasets
print(KaggleDatasets().get_gcs_path())
```

比如房价预测竞赛[House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/)，其得到的结果如下：

`gs://kds-ecc57ad1aae587b0e86e3b9422baab9785fc1220431f0b88e5327ea5`

现在就可以通过 `gcsfuse` 来挂载 GCS：

```text
! mkdir -p <CompetitionID>
! gcsfuse  --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 <GCSPath without gs://> <CompetitionID>
```

通过上述命令进行挂载，1s 内即可完成。但当你开始尝试在数据集上进行迭代操作的时候，你会发现磁盘访问速度是非常的慢。访问速度是依赖于 Colab 实例和 GCS 桶 所在的区域，但一般来说，应该避免这种挂载操作。

可以通过以下方式获取Colab实例的区域信息:

```shell
! curl ipinfo.io
```

GCS 桶的区域信息本来是可以通过下述命令获取的，但是我这边提示 `AccessDeniedException` 异常错误，并且无法解决。

```text
! gsutil ls -Lb gs://kds-ecc57ad1aae587b0e86e3b9422baab9785fc1220431f0b88e5327ea5
```



### 挂载 Google D磁盘

这种方法的磁盘访问速度太慢了！



------

## 总结

使用 Colab Pro 是能够用到更好的显卡，比如 TPU，或者 P100，V100 这个级别的 GPU，当然这是一个付费版本，每个月需要花费 10 美元，而且每次会话运行时间最长只有 24 小时，所以如果训练模型时间太久，就需要多次运行会话，这就导致需要重新加载数据集和读取上次训练保存的权重文件。

因此为了最大化利用 Colab Pro 的时间，当然就希望减少加载数据集的时间，本文作者基于这个思路，对比了 5 种方法，最后是根据实际情况，即主要是训练模型，对磁盘读取速度要求更高，所以选择了在 Google 磁盘上解压缩文件到 Kaggle 实例上的方法，并给出了操作流程，然后还研究了是否可以通过挂载外部存储器，但磁盘访问速度太慢，并不建议这种操作。

另外，Colab Pro 提供的磁盘空间仅有 150GB，对于压缩文件大小是不能超过 75GB，因此，这种方法对于大数据集，参数量很大的网络模型都是不太合适的，但对于数据量不大，不用太大的网络模型的比赛，还是可以用 Colab Pro 来训练模型。