# 02. 数学基础学习笔记--线性代数

开始复习 AI 算法的基础--数学部分，主要是三方面的内容：

- 线性代数

- 概率论

- 微积分

参考内容如下：

1. Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017
2. 深度学习 500 问第一章数学基础：https://github.com/scutan90/DeepLearning-500-questions/tree/master/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80
3. [Reflection_Summary--数学](https://github.com/sladesha/Reflection_Summary/tree/master/%E6%95%B0%E5%AD%A6)







# 1.线性代数

## 1.1 向量和矩阵

### 1.1.1 标量、向量、矩阵、张量之间的联系

**标量（scalar）** 

一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 一般会明确标量属于哪种类型，比如定义实数标量时，会说“令 $s\in R$ 表示一条线的斜率”。

**向量（vector）** 

一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量$X$的第一个元素是$X_1$，第二个元素是$X_2$，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。

一个向量如下所示，一个向量可以看作空间中的点，即每个元素可以表示不同坐标轴上的坐标。
$$
x = 
 \left[
 \begin{matrix}
   x_1 \\
   x_2 \\
   x_3 \\
   \cdots \\
   x_n
  \end{matrix}
  \right] 
$$




**矩阵（matrix）** 

矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如$A$。

一个矩阵的表示例子如下所示：
$$
A = 
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
  \end{matrix}
  \right] 
$$

**转置**是矩阵的重要操作之一，其转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为**主对角线**，定义如下:
$$
(A^T){i,j} = A_{j,i} 
$$
一个示例操作如下：
$$
A = 
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
   A_{3,1} & A_{3,2}
  \end{matrix}
  \right]
  ==> 
  A^T = 
  \left[
 \begin{matrix}
   A_{1,1} & A_{2,1} & A_{3, 1} \\
   A_{1,2} & A_{2,2} & A_{3,2}\\
  \end{matrix}
  \right] 
$$

从一个 $3\times 2$ 的矩阵变为了 $ 2\times 3$ 的矩阵。



**张量（tensor）** 

在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 $A$ 来表示张量“A”。张量$A$中坐标为$(i,j,k)$的元素记作$A_{(i,j,k)}$。 



**四者之间关系**  

（来自深度学习 500 问第一章数学基础）

> 标量是0阶张量，向量是一阶张量。举例：  
> 标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。  
> 向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。  
> 张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。





### 1.1.2 张量与矩阵的区别

- 从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么$n$阶张量就是所谓的$n$维的“表格”。 张量的严格定义是利用线性映射来描述。
- 从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。
- 张量可以用3×3矩阵形式来表达。 
- 表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。 



### 1.1.3 矩阵和向量相乘结果   

若使用爱因斯坦求和约定（Einstein summation convention），矩阵$A$, $B$相乘得到矩阵 $C$ 可以用下式表示：
$$ AB = C ==> a_{ik}*b_{kj}=c_{ij}  $$ 

其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分别表示矩阵$A, B, C$的元素，$k$出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。

用一个例子表示就是：
$$
A=
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
  \end{matrix}
  \right] \
  B = 
  \left[
 \begin{matrix}
   B_{1,1} & B_{1,2} \\
   B_{2,1} & B_{2,2} \\
  \end{matrix}
  \right] \\
  A \times B = C = 
  \left[
 \begin{matrix}
   A_{1,1}\times B_{1,1}+A_{1,2}\times B_{2,1} & A_{1,1}\times B_{1,2}+A_{1,2}\times B_{2,2} \\
   A_{2,1}\times B_{1,1}+A_{2,2}\times B_{2,1} & A_{2,1}\times B_{1,2}+A_{2,2}\times B_{2,2} \\
  \end{matrix}
  \right]
  = 
   \left[
 \begin{matrix}
   C_{1,1} & C_{1,2} \\
   C_{2,1} & C_{2,2} \\
  \end{matrix}
  \right]
$$
所以矩阵相乘有一个前提，**矩阵 A 的列数必须和矩阵 B 的行数相等**，也就是如果 A 的维度是 $m\times n$，B 的维度必须是 $n \times p$，相乘得到的 C 矩阵的维度就是 $m\times p$。

另外还有一种矩阵乘法，是矩阵对应元素相乘，这种称为**元素对应乘积，或者 Hadamard 乘积**，记为 A ⊙ B


而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵$B$是一个$n \times 1$的矩阵。



矩阵乘积满足这些定律：

1. 服从分配率：A(B+C) = AB + AC
2. 服从结合律：A(BC) = (AB)C

但是**不服从交换律**，即 AB 不一定等于 BA。

矩阵的乘积满足：$（AB)^T = A^TB^T$



两个相同维度的向量 x 和 y 的点积(dot product)，可以看作矩阵乘积--$x^Ty$。也就是说可以将矩阵乘积 $C=AB$ 中计算 $C_{i,j}$的步骤看作是 A 的第 i 行和 B 的第 j 列之间的点积。毕竟，矩阵的每一行或者每一列都是一个向量。

而向量的点积是满足交换律的：
$$
x^Ty = y^Tx 
$$
证明主要是根据：

1. 两个向量的点积是标量
2. 标量的转置也是自身

所以有：
$$
x^Ty = (x^Ty)^T = xy^T 
$$

### 1.1.4 单位矩阵和逆矩阵

单位矩阵的定义如下，**用 I 表示单位矩阵，任何向量和单位矩阵相乘，都不会改变**，即：
$$
\forall x \in R^n, I_n x = x \tag{1-1-8}
$$
单位矩阵的结构很简单，就是主对角线是 1，其他位置是 0，如下图所示的单位矩阵 $I_3$ ：
$$
\left[
 \begin{matrix}
   1 & 0 & 0 \\
   0 & 1 & 0 \\
   0 & 0 & 1
  \end{matrix}
  \right]
$$
而逆矩阵记作 $A^{-1}$，其满足如下条件：
$$
A^{-1}A=I_n 
$$



### 1.1.5 线性方程组和线性相关

现在有一个线性方程组，如下所示：
$$
Ax = b
$$
其中，$A\in R^{m\times n}$ 是已知的矩阵，$b\in R^m$ 是已知的向量，然后 $x\in R^n$ 是需要求解的未知向量。

这里根据矩阵相乘（x 相当于一个 $n\times 1$ 的矩阵），可以将上述公式拓展开来：
$$
A_{1,:}x = b_1 ==> A_{1,1}x_1 + A_{1,2}x_2+\cdots+A_{1,n}x_n = b_1 \\
A_{2,:}x = b_2 ==> A_{2,1}x_1 + A_{2,2}x_2+\cdots+A_{2,n}x_n = b_2 \\
\cdots \\
A_{m,:}x = b_m ==> A_{m,1}x_1 + A_{m,2}x_2+\cdots+A_{m,n}x_n = b_m \\
$$
在我们定义了逆矩阵后，那么可以这么求解：
$$
Ax=b\\
A^{-1}Ax = A^{-1}b\\
I_nx = A^{-1}b \\
x = A^{-1}b
$$
所以求解的关键就是是否存在一个逆矩阵，并找到它。

当逆矩阵$A^{-1}$存在的时候，对每个向量 b 肯定恰好存在一个解。

但对于方程组来说，向量 b 的某些值，有可能不存在解，或者有无限多个解，不存在多于1 个解，但有限解的情况，比如 x 和 y 都是方程组的解，则有：
$$
z = \alpha x + (1-\alpha)y
$$
其中，$\alpha$ 是任意实数，那么 z 也是方程组的解，这种组合是无限的，所以不存在有限解（多于 1 个）。

确定 Ax=b 是否有解，**关键是确定向量 b 是否在 A 列向量的生成子空间中**，这个特殊的生成子空间，被称为 A 的列空间或者 A 的值域。

> 一组向量的线性组合是指每个向量乘以对应标量系数之后的和，即 $\sum_i c_i v^{(i)}$
>
> 一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合。

那么为了让上述成立，**应该让 A 的列空间构成整个 $R^m$ 空间**，如果这个空间某个点不在 A 的列空间，那么对应的 b 会使得方程无解。而要让其成立，**即要满足不等式 $n\ge m$ **。

但该不等式只是方程对每个 b 有解的必要条件，非充分条件。因为存在一种情况，某些列向量可能是冗余的，比如一个 $2\times 2$的矩阵，如果两个列向量都是相同的，那该矩阵的列空间和它的一个列向量作为矩阵的列空间是一样的，并不能满足覆盖了整个 $R^2$ 空间。

这种冗余也被称为**线性相关**，而**如果一组向量中任意一个向量都不能表示为其他向量的线性组合，则这组向量称为线性无关**。

所以，**如果一个矩阵的列空间要覆盖整个 $R^m$，那么该矩阵必须包含至少一组m 个线性无关的向量，这才是对每个 b 都有解的充分必要条件**。

此外，要**让矩阵可逆**，还必须保证 Ax=b 对每个 b 的取值至多只有一个解，那必须保证该矩阵至多有 m 个列向量，否则方程有不止一个解。

综上，**那么矩阵就必须是方阵，也就是 m = n，并且所有列向量都是线性无关的**。一个列向量都是线性无关的方阵被称为是**奇异的**。

假如 A 不是方阵或者不是奇异的方阵，也可能有解，但是不能通过逆矩阵去求解。






### 1.1.6 向量和矩阵的范数归纳  
**向量的范数(norm)** 

通常衡量向量的大小是通过**范数**来衡量的，形式上 $L^P$范数定义如下：

$$
L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}
$$

这里 $p\ge 1$。

范数是将向量映射到非负数的函数，直观上来说，向量 x 的范数衡量从原点到点 x 的距离。

范数是满足下列性质的任意函数：
$$
f(x)=0=>x=0 \\
f(x+y)\le f(x)+f(y)(三角不等式)\\
\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)
$$




定义一个向量为：$\vec{a}=[-5, 6, 8, -10]$。任意一组向量设为$\vec{x}=(x_1,x_2,...,x_N)$。其不同范数求解如下：

- 向量的1范数：向量的各个元素的绝对值之和，上述向量$\vec{a}$的1范数结果就是：x = |-5|+|6|+|8|+|-10| = 29。
$$
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert
$$

- 向量的2范数（欧几里得范数）：向量的每个元素的平方和再开平方根，上述$\vec{a}$的2范数结果就是：$x=\sqrt{(-5)^2+(6)^2+(8)^2+(-10)^2}15$。
$$
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}
$$

- 向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量$\vec{a}$的负无穷范数结果就是：5。  
$$
\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}
$$

- 向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量$\vec{a}$的正无穷范数结果就是：10。 
$$
\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}
$$





**矩阵的范数**  

定义一个矩阵。 
$$
A =
\left[
 \begin{matrix}
   -1 & 2 & -3 \\
   4 & -6 & 6 \\
  \end{matrix}
  \right]
$$


任意矩阵定义为：$A_{m\times n}$，其元素为 $a_{ij}$。

矩阵的范数定义为

$$
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}
$$

当向量取不同范数时, 相应得到了不同的矩阵范数。

- **矩阵的1范数（列范数）**：先对矩阵的每一列元素的绝对值求和，再从中取个最大的（列和最大），上述矩阵$A$的1范数先得到$[5,8,9]$，再取最大的最终结果就是：9。
$$
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|
$$

- **矩阵的2范数**：矩阵$A^TA$的最大特征值开平方根，上述矩阵$A$的2范数得到的最终结果是：10.0623。 
$$
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}
$$

其中， $\lambda_{max}(A^T A)$ 为 $A^T A$ 的特征值绝对值的最大值。
- **矩阵的无穷范数（行范数）**：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵$A$的行范数先得到$[6；16]$，再取最大的最终结果就是：16。 
$$
\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|
$$

- **矩阵的核范数**：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。  

- **矩阵的L0范数**：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵$A$最终结果就是：6。
- **矩阵的L1范数**：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵$A$最终结果就是：22。  
- **矩阵的F范数**：最常用的矩阵的范数，矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，**它的优点在于它是一个凸函数，可以求导求解，易于计算**，上述矩阵A最终结果就是：10.0995。  
$$
\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}
$$

- **矩阵的L21范数**：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵$A$最终结果就是：17.1559。 
- **矩阵的 p范数** 
$$
\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}
$$

两个向量的点积可以用范数来表示：
$$
x^Ty =\Vert x \Vert_2 \Vert y \Vert_2 cos\theta
$$
这里 $\theta$ 就是 x 和 y 之间的夹角。



### 1.1.7 一些特殊的矩阵和向量

**对角矩阵**：只在对角线上有非零元素，其他位置都是零。之前介绍的单位矩阵就是对角矩阵的一种；

**对称矩阵**：转置和自己相等的矩阵，即：$A = A^T$。

**单位向量**：具有单位范数的向量，也就是 $\Vert x \Vert_2 =1$



**向量正交**：如果 $x^Ty=0$，那么就说向量 x 和 y 互相正交。如果向量不仅互相正交，范数还是 1，那么就称为**标准正交**。



**正交矩阵**：行向量和列向量是分别标准正交的方阵，即
$$
A^TA=AA^T=I
$$
也就是有：
$$
A^{-1}=A^T
$$
所以正交矩阵的一个优点就是求逆计算代价小。



### 1.1.8 如何判断一个矩阵为正定

判定一个矩阵是否为正定，通常有以下几个方面：  

- 顺序主子式全大于0；  
- 存在可逆矩阵$C$使$C^TC$等于该矩阵；
- 正惯性指数等于$n$；
- 合同于单位矩阵$E$（即：规范形为$E$）
- 标准形中主对角元素全为正；
- 特征值全为正；
- 是某基的度量矩阵。



所有特征值是非负数的矩阵称为半正定，而所有特征值是负数的矩阵称为负定，所有特征值是非正数的矩阵称为半负定。

**正定性的用途**

- Hessian矩阵正定性在梯度下降的应用
  - 若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解
- 在 svm 中核函数构造的基本假设





## 1.2 特征值和特征向量

### 1.2.1 特征值分解与特征向量  

**特征分解是使用最广的矩阵分解之一**，矩阵分解可以得到一组特征值(eigenvalues)与特征向量(eigenvectors)；



特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。  

如果说一个向量$\vec{v}$是方阵$A$的特征向量，将一定可以表示成下面的形式：

$$
A\nu = \lambda \nu
$$

$\lambda$为特征向量$\vec{v}$对应的特征值。

特征值分解是将一个矩阵分解为如下形式： 
    
$$
A=Q\sum Q^{-1}
$$

其中，$Q$是这个矩阵$A$的**特征向量组成的正交矩阵**，$\sum$是一个对角矩阵，**每一个对角线元素就是一个特征值**，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵$A$的信息可以由其特征值和特征向量表示。

并非每个矩阵都可以分解成特征值和特征向量，但**每个实对称矩阵**都可以分解为实特征向量和实特征值。



### 1.2.2 奇异值分解

除了特征分解外，还有一种矩阵分解，称为**奇异值分解**（SVD)，将矩阵分解为奇异值和奇异向量。通过奇异值分解，可以得到和特征分解相同类型的信息，但是，奇异值分解有更广泛的应用，**每个实数矩阵都有一个奇异值分解，但不一定有特征分解，因为必须是方阵才有特征分解**。

在特征分解中，我们将 A 重新写作：
$$
A = Vdiag(\lambda)V^{-1}
$$
其中，V 是特征向量构成的矩阵，$\lambda$是特征值构成的向量，$diag(\lambda)$表示一个对角线都是特征值的对角矩阵。

奇异值分解的形式如下所示：
$$
A = U D V^T
$$
假如 A 是 $m\times n$ 的矩阵，则 U 是 $m\times m$的矩阵，D 是 $m\times n$ 的矩阵，V 是 $n\times n$ 的矩阵。并且，矩阵 U 和 V 是正交矩阵，D 是对角矩阵，且不一定是方阵。

**D 对角线上的元素就是 A 的奇异值，而 U 的列向量是左奇异向量，V 的列向量是右奇异向量。**

可以套用和 A 相关的特征分解来解释其奇异值分解，A 的左奇异向量就是 $AA^T$的特征向量，而右奇异向量就是$A^TA$ 的特征向量，A 的非零奇异值是$AA^T$特征值的平方根，也是$A^TA$特征值的平方根。



(来自深度学习 500 问的数学基础的内容)

> 那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵$A$的转置乘以$A$，并对$A^TA$求特征值，则有下面的形式：
>
> $$
> (A^TA)V = \lambda V
> $$
>
> 这里$V$就是上面的右奇异向量，另外还有：
>
> $$
> \sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}AV
> $$
>
> 这里的$\sigma$就是奇异值，$u$就是上面说的左奇异向量。



奇异值$\sigma$跟特征值类似，在矩阵$\sum$中也是从大到小排列，而且$\sigma$的减少特别的快，**在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了**。也就是说，我们也可以用前$r$（$r$远小于$m、n$）个的奇异值来近似描述矩阵，即部分奇异值分解：
$$
A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T
$$

右边的三个矩阵相乘的结果将会是一个接近于$A$的矩阵，在这儿，**$r$越接近于$n$，则相乘的结果越接近于$A$**。









------

# 参考

1. Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017
2. 深度学习 500 问第一章数学基础：https://github.com/scutan90/DeepLearning-500-questions/tree/master/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80
3. [Reflection_Summary--数学](