# 04. 数学基础学习笔记--数值计算

# 3. 数值计算

## 3.1 上溢和下溢

连续数学在计算机上的根本困难是：**需要通过有限数量的位模式来表示无限多的实数**。

这意味着在计算机中表示实数的时候几乎总是会引入一些近似误差，许多情况下这仅仅是**舍入误差**。但如果没有考虑最小化舍入误差的累积，可能就会导致算法的失效。这里存在两种比较严重的情况：

1. **下溢**：当接近 0 的数被四舍五入为零时发生下溢的情况。许多函数在参数为 0 而不是一个很小的正数的时候会表现出质的不同。例如，我们通常要避免被零除（有些软件会返回异常，或者会抛出一个非数字，比如 NaN 的占位符）或者避免取零的对数（这通常被视为负无穷）
2. **上溢**：当大量级的数被近似为 $\infin$ 或者 $-\infin$ 。进一步的运算会导致这些无限制变为非数字，比如 inf

必须对上溢和下溢进行数值稳定的一个例子是 **softmax 函数**，它经常用于预测和 Multinoulli 分布相关联的概率，也是 CNN 里常用的函数，其定义如下：
$$
softmax(x)_i = \frac{exp(x_i)}{\sum^n_{j=1}exp(x_j)}
$$
假如 x 是等于一个常数 c，那么输出应该是 $\frac{1}{n}$ ，但有两种情况需要注意：

1. c 是一个很小的负数，会发生下溢的情况；
2. c 是一个很大的正数，会发现上溢的情况

解决的办法，是先做一个简单的变换：
$$
z = x-max_i(x_i)
$$
然后计算 softmax(z) 。

也就是先让每个 x 减去最大值，那么这让exp 的参数最大值为 0（当 x 取最大值的时候），这排除了上溢的可能性；而分母中至少有一项是 1（x 取最大值，那么 exp(z)=1），这也排除了下溢的可能性，即分母不会等于0.

但这里还有另一个问题就是分子的下溢导致结果是 0，这种情况如果计算 log softmax(x) 会得到$-\infin$ 的结果。







##  3.2 导数和偏导数

### 3.2.1 导数和偏导计算

**导数定义**:

导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。

- 几何意义是这个点的切线。
- 物理意义是**该时刻的（瞬时）变化率**。

*注意*：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有
$$
v=\frac{s}{t}
$$

其中$v$表示平均速度，$s$表示路程，$t$表示时间。这个公式可以改写为

$$
\bar{v}=\frac{\Delta s}{\Delta t}=\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}
$$

其中$\Delta s$表示两点之间的距离，而$\Delta t$表示走过这段距离需要花费的时间。当$\Delta t$趋向于0（$\Delta t \to 0$）时，也就是时间变得很短时，平均速度也就变成了在$t_0$时刻的瞬时速度，表示成如下形式：

$$
v(t_0)=\lim_{\Delta t \to 0}{\bar{v}}=\lim_{\Delta t \to 0}{\frac{\Delta s}{\Delta t}}=\lim_{\Delta t \to 0}{\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}}
$$

实际上，上式表示的是路程$s$关于时间$t$的函数在$t=t_0$处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有

$$
\lim_{\Delta x \to 0}{\frac{\Delta y}{\Delta x}}=\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}}
$$

则称此极限为函数 $y=f(x)$ 在点 $x_0$ 处的导数。记作 $f'(x_0)$ 或 $y'\vert_{x=x_0}$ 或 $\frac{dy}{dx}\vert_{x=x_0}$ 或 $\frac{df(x)}{dx}\vert_{x=x_0}$。

通俗地说，导数就是曲线在某一点切线的斜率。



**偏导数**:

既然谈到偏导数(partial derivative)，**那就至少涉及到两个自变量**。以两个自变量为例，$z=f(x,y)$，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。 


*注意*：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。

设函数$z=f(x,y)$在点$(x_0,y_0)$的领域内有定义，当$y=y_0$时，$z$可以看作关于$x$的一元函数$f(x,y_0)$，若该一元函数在$x=x_0$处可导，即有

$$
\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}}=A
$$

函数的极限$A$存在。那么称$A$为函数$z=f(x,y)$在点$(x_0,y_0)$处关于自变量$x$的偏导数，记作$f_x(x_0,y_0)$或$\frac{\partial z}{\partial x}\vert_{y=y_0}^{x=x_0}$或$\frac{\partial f}{\partial x}\vert_{y=y_0}^{x=x_0}$或$z_x\vert_{y=y_0}^{x=x_0}$。

偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如$z=3x^2+xy$关于$x$的偏导数就为$z_x=6x+y$，这个时候$y$相当于$x$的系数。

某点$(x_0,y_0)$处的偏导数的几何意义为曲面$z=f(x,y)$与面$x=x_0$或面$y=y_0$交线在$y=y_0$或$x=x_0$处切线的斜率。  



### 3.2.2 导数和偏导数有什么区别？  

导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。  

> - 一元函数，一个$y$对应一个$x$，导数只有一个。  
> - 二元函数，一个$z$对应一个$x$和一个$y$，有两个导数：一个是$z$对$x$的导数，一个是$z$对$y$的导数，称之为偏导。  
> - 求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。



### 3.2.3 导数的四则运算



- $(u+v)^{'}=u^{'}+v^{'}$
- $(u-v)^{'}=u^{'}-v^{'}$
- $(uv)^{'} = u^{'}v+uv^{'}$
- $(u/v)^{'}=\frac{u^{'}v-uv^{'}}{v^2}$



### 3.2.4 常见的导数运算法则

- y=c(常数) ==> $y^{'}=0$
- $y=x^a$  ==> $y^{'}=ax^{a-1}$
- $y=a^x$ ==> $y^{'}=ln(a)a^x$
- $y=log_ax$==>$y^{'} = \frac{1}{xlna}$
- $y = lnx$ ==> $y^{'}=\frac{1}{x}$
- $y=sin(x)$ ==> $y^{'}=cos(x)$
- $y=cos(x)$ ==> $y^{'}=-sin(x)$
- $y=tan(x)$ ==> $y^{'}=\frac{1}{cos(x)^2}$



### 3.2.5 复合函数的运算法则

假设 g 在 x 处可导，且 f 在 g(x) 处可导，那么 y=f(g(x)) 的导数计算：
$$
y^{'} = f^{'}(g(x))\cdot g^{'}(x)
$$






## 3.3 基于梯度的优化方法

大多数深度学习算法都涉及某种形式的优化。**优化指的是改变 x 以最小化或者最大化某个函数 f(x) 的任务**。

通常我们都用最小化 f(x) 指代大多数最优化问题，最大化可以由最小化 -f(x) 来实现。

一般将需要优化的函数称为**目标函数(object function)或者准则(criterion)**，而如果是其进行最小化的时候，也称为**代价函数(cost function)、损失函数(loss function)或者误差函数(error function)**。

一般用一个上标 * 表示最小化或者最大化函数的 x 的取值，比如 $x^* = argmin f(x)$。

假设有一个函数 y=f(x)，其导数记作 $f^{'}(x)$或者 $\frac{dy}{dx}$

导数对于最小化是非常有用的，因为它告诉我们如何更改 x 来略微地改善 y。我们可以根据导数来决定对 x 的修改，是增大 x 还是减小 x，一般是移动 x 来走到导数为 0 的位置，从而最小化 y，这种做法就是**梯度下降**。

对于 $f^{'}=0$的点，称为**临界点或者驻点**。这个点可能是局部的极小点或者极大点，也可能是全局最小点或者最大点，也可能是一个鞍点，也就是既不是最大点也不是最小点。

理想情况下，当然是希望达到全局极小点，但这可能达不到，通常是达到局部极小点，但有的局部极小点非常接近全局最小点，有的极小点可能非常大，所以要尽量达到全局极小点或者接近的局部极小点。



**梯度**是相对一个向量求导的导数，f 的导数是包含所有偏导数的向量，记为 $\nabla_xf(x)$，梯度的第 i 个元素是 f 关于 $x_i$ 的偏导数。在多维情况下，临界点是梯度中所有元素都是 0 的点。

在负梯度上移动可以减少 f，这称为**最速下降法或者梯度下降**，它建议新的点为：
$$
x^{'}=x-\alpha \nabla_x f(x)
$$
这里 $\alpha$ 是学习率，是一个确定步长的正标量，通常其初始值是一个比较小的常数。





------

# 参考

1. Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017
2. 深度学习 500 问第一章数学基础：https://github.com/scutan90/DeepLearning-500-questions/tree/master/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80
3. [Reflection_Summary--数学](https://github.com/sladesha/Reflection_Summary/tree/master/%E6%95%B0%E5%AD%A6)