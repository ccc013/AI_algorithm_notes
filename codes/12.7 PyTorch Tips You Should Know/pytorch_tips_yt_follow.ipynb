{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tips_yt_follow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYpb4rz9GDFx"
      },
      "source": [
        "# 7 PyTorch Tips You Should Know"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdIxsiZf-mAS"
      },
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEXFLOIkAlrF"
      },
      "source": [
        "# 1. Create Tensors Directly on the Target Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i0i1gk4-RkL",
        "outputId": "c202a2c9-1d9c-49e5-cb70-ef01bf75e228"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for _ in range(100):\n",
        "  # Creating on the CPU, then transfering to the GPU\n",
        "  cpu_tensor = torch.ones((1000, 64, 64))\n",
        "  gpu_tensor = cpu_tensor.cuda()\n",
        "\n",
        "print('Total time: {:.3f}s'.format(time.time() - start_time))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 11.250s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69pMZsQs-Sto",
        "outputId": "0c481ac1-d065-44f4-e8d0-708fec7f6393"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for _ in range(100):\n",
        "  # Creating on GPU directly\n",
        "  cpu_tensor = torch.ones((1000, 64, 64), device='cuda')\n",
        "\n",
        "print('Total time: {:.3f}s'.format(time.time() - start_time))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 0.007s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta-sxuoPAyJQ"
      },
      "source": [
        "# 2. Use `Sequential` Layers When Possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2r4DXtL-RPP"
      },
      "source": [
        "class ExampleModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    input_size = 2\n",
        "    output_size = 3\n",
        "    hidden_size = 16\n",
        "\n",
        "    self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "    self.input_activation = nn.ReLU()\n",
        "\n",
        "    self.mid_layer = nn.Linear(hidden_size, hidden_size)\n",
        "    self.mid_activation = nn.ReLU()\n",
        "\n",
        "    self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.input_layer(x)\n",
        "    z = self.input_activation(z)\n",
        "    \n",
        "    z = self.mid_layer(z)\n",
        "    z = self.mid_activation(z)\n",
        "    \n",
        "    out = self.output_layer(z)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOf_q_JT-V82",
        "outputId": "91353470-be33-48ea-9e15-1c7ed05876d5"
      },
      "source": [
        "example_model = ExampleModel()\n",
        "print(example_model)\n",
        "print('Output shape:', example_model(torch.ones([100, 2])).shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExampleModel(\n",
            "  (input_layer): Linear(in_features=2, out_features=16, bias=True)\n",
            "  (input_activation): ReLU()\n",
            "  (mid_layer): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (mid_activation): ReLU()\n",
            "  (output_layer): Linear(in_features=16, out_features=3, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([100, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCHY0KtT-WGj"
      },
      "source": [
        "class ExampleSequentialModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    input_size = 2\n",
        "    output_size = 3\n",
        "    hidden_size = 16\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size, hidden_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_size, output_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.layers(x)\n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1PVgMcT-WNT",
        "outputId": "8b43a487-fa52-42b4-db58-0fa12783c210"
      },
      "source": [
        "example_seq_model = ExampleSequentialModel()\n",
        "print(example_seq_model)\n",
        "print('Output shape:', example_seq_model(torch.ones([100, 2])).shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ExampleSequentialModel(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=16, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "Output shape: torch.Size([100, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLGRA4CyAztx"
      },
      "source": [
        "# 3. Don't Make Lists of Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI2xZ3EP-Xkp"
      },
      "source": [
        "class BadListModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    input_size = 2\n",
        "    output_size = 3\n",
        "    hidden_size = 16\n",
        "\n",
        "    self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "    self.input_activation = nn.ReLU()\n",
        "\n",
        "    # Fairly common when using residual layers\n",
        "    self.mid_layers = []\n",
        "    for _ in range(5):\n",
        "      self.mid_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "      self.mid_layers.append(nn.ReLU())\n",
        "\n",
        "    self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.input_layer(x)\n",
        "    z = self.input_activation(z)\n",
        "    \n",
        "    for layer in self.mid_layers:\n",
        "      z = layer(z)\n",
        "    \n",
        "    out = self.output_layer(z)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48GAPovU-Xe1",
        "outputId": "09e19788-e31a-474e-b1a2-626120fcd865"
      },
      "source": [
        "bad_list_model = BadListModel()\n",
        "print('Output shape:', bad_list_model(torch.ones([100, 2])).shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([100, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "t6n32oG_-XXp",
        "outputId": "427edcfe-7e37-4ec7-d4bb-c69afe905d34"
      },
      "source": [
        "gpu_input = torch.ones([100, 2], device='cuda')\n",
        "gpu_bad_list_model = bad_list_model.cuda()\n",
        "print('Output shape:', bad_list_model(gpu_input).shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e523900f19d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgpu_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgpu_bad_list_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbad_list_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_list_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2df20007fc89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensor for 'out' is on CPU, Tensor for argument #1 'self' is on CPU, but expected them to be on GPU (while checking arguments for addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9z0YF7oGZXf"
      },
      "source": [
        "## Better Way to Do This"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnnyjZp3-Y0a"
      },
      "source": [
        "class CorrectListModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    input_size = 2\n",
        "    output_size = 3\n",
        "    hidden_size = 16\n",
        "\n",
        "    self.input_layer = nn.Linear(input_size, hidden_size)\n",
        "    self.input_activation = nn.ReLU()\n",
        "\n",
        "    # Fairly common when using residual layers\n",
        "    self.mid_layers = []\n",
        "    for _ in range(5):\n",
        "      self.mid_layers.append(nn.Linear(hidden_size, hidden_size))\n",
        "      self.mid_layers.append(nn.ReLU())\n",
        "    self.mid_layers = nn.Sequential(*self.mid_layers)\n",
        "\n",
        "    self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.input_layer(x)\n",
        "    z = self.input_activation(z)\n",
        "    z = self.mid_layers(z)\n",
        "    out = self.output_layer(z)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJt9-A5a-Ys5",
        "outputId": "1d80248e-66bf-4b88-bfdc-53f534c0e3b9"
      },
      "source": [
        "correct_list_model = CorrectListModel()\n",
        "gpu_input = torch.ones([100, 2], device='cuda')\n",
        "gpu_correct_list_model = correct_list_model.cuda()\n",
        "print('Output shape:', correct_list_model(gpu_input).shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([100, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTlaVIitAzxJ"
      },
      "source": [
        "# 4. Make Use of Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNgAZKGh-bgX",
        "outputId": "c06cb8e5-9571-437c-b9cc-027de925dcd1"
      },
      "source": [
        "# Setup\n",
        "example_model = ExampleModel()\n",
        "input_tensor = torch.rand(5, 2)\n",
        "output = example_model(input_tensor)\n",
        "print(output)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1751, 0.3160, 0.0865],\n",
            "        [0.1616, 0.2483, 0.0599],\n",
            "        [0.1604, 0.2266, 0.0476],\n",
            "        [0.1606, 0.2384, 0.0569],\n",
            "        [0.1193, 0.2597, 0.0577]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgyPL1Gn-baX"
      },
      "source": [
        "from torch.distributions import Categorical\n",
        "from torch.distributions.kl import kl_divergence"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCU8D-mO-bTO",
        "outputId": "463d48fb-3733-4397-ef70-50a45c74adb1"
      },
      "source": [
        "dist = Categorical(logits=output)\n",
        "dist"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Categorical(logits: torch.Size([5, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCkCQyY-bKq",
        "outputId": "e42be925-66cc-4115-a041-3007b4a15878"
      },
      "source": [
        "# Get probabilities\n",
        "dist.probs"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3261, 0.3755, 0.2984],\n",
              "        [0.3340, 0.3643, 0.3017],\n",
              "        [0.3376, 0.3608, 0.3016],\n",
              "        [0.3353, 0.3624, 0.3023],\n",
              "        [0.3235, 0.3723, 0.3042]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAnfI0Dt-bEo",
        "outputId": "f3c02efd-c48d-477e-f925-142d285cf509"
      },
      "source": [
        "# Take samples\n",
        "dist.sample()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1, 1, 1, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-BFWjGo-a9M",
        "outputId": "e7ddecff-4e19-4292-89b7-2de669a1314c"
      },
      "source": [
        "# Calculate the KL-Divergence\n",
        "dist_1 = Categorical(logits=output[0])\n",
        "dist_2 = Categorical(logits=output[1])\n",
        "kl_divergence(dist_1, dist_2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0003, grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qZwgkkjAzz-"
      },
      "source": [
        "# 5. Use `detach()` On Long-Term Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN-ZgkpX-dCG",
        "outputId": "a756ed20-6e62-449e-9c1b-c8beb7cac4e3"
      },
      "source": [
        "# Setup\n",
        "example_model = ExampleModel()\n",
        "data_batches = [torch.rand((10, 2)) for _ in range(5)]\n",
        "criterion = nn.MSELoss(reduce='mean')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5tRrcMXKb4i"
      },
      "source": [
        "## Bad Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0WYtsqx-cwX",
        "outputId": "e8ba253f-5892-4922-a31b-248e4e5b8556"
      },
      "source": [
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for batch in data_batches:\n",
        "  output = example_model(batch)\n",
        "\n",
        "  target = torch.rand((10, 3))\n",
        "  loss = criterion(output, target)\n",
        "  losses.append(loss)\n",
        "\n",
        "  # Optimization happens here\n",
        "\n",
        "print(losses)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(0.3803, grad_fn=<MseLossBackward>), tensor(0.2849, grad_fn=<MseLossBackward>), tensor(0.3214, grad_fn=<MseLossBackward>), tensor(0.2970, grad_fn=<MseLossBackward>), tensor(0.2953, grad_fn=<MseLossBackward>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P685bJgJvzn"
      },
      "source": [
        "## Better Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4zbLSpT-dbu",
        "outputId": "c4a21c70-2cfa-43b6-b2a5-dadf450c3840"
      },
      "source": [
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for batch in data_batches:\n",
        "  output = example_model(batch)\n",
        "\n",
        "  target = torch.rand((10, 3))\n",
        "  loss = criterion(output, target)\n",
        "  losses.append(loss.item()) # Or `loss.item()`\n",
        "\n",
        "  # Optimization happens here\n",
        "\n",
        "print(losses)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.42216455936431885, 0.44215714931488037, 0.36152341961860657, 0.42846500873565674, 0.38867220282554626]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkSVHK38Az3F"
      },
      "source": [
        "# 6. Trick to Delete a Model from GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CocJHuhl-e_V"
      },
      "source": [
        "import gc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKlBHS-D-e4F"
      },
      "source": [
        "example_model = ExampleModel().cuda()\n",
        "\n",
        "del example_model\n",
        "\n",
        "gc.collect()\n",
        "# The model will normally stay on the cache until something takes it's place\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uagavyWeAz6K"
      },
      "source": [
        "# 7. Call `eval()` Before Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgkGWVcU-fgp",
        "outputId": "21c29bc6-b30c-4eec-cceb-6a0dc0c21dcd"
      },
      "source": [
        "example_model = ExampleModel()\n",
        "\n",
        "# Do training\n",
        "\n",
        "example_model.eval()\n",
        "\n",
        "# Do testing\n",
        "\n",
        "example_model.train()\n",
        "\n",
        "# Do training again"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExampleModel(\n",
              "  (input_layer): Linear(in_features=2, out_features=16, bias=True)\n",
              "  (input_activation): ReLU()\n",
              "  (mid_layer): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (mid_activation): ReLU()\n",
              "  (output_layer): Linear(in_features=16, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnvexQJY-gyH"
      },
      "source": [
        "### Affects\n",
        "  - Dropout\n",
        "  - Batch Normalization\n",
        "  - RNNs\n",
        "  - Lazy Variants\n",
        "\n",
        "source: https://stackoverflow.com/questions/66534762/which-pytorch-modules-are-affected-by-model-eval-and-model-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c28JegUPEBdl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}